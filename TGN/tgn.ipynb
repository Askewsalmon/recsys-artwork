{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(\"../dataset/users.csv\", sep=\";\")\n",
    "artworks_df = pd.read_csv(\"../dataset/artworks.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for idx, row in artworks_df.iterrows():\n",
    "    for col in artworks_df.columns:\n",
    "        if col.startswith(\"comment_\") and not pd.isna(row[col]):\n",
    "            comment = eval(row[col])\n",
    "            for c in comment:\n",
    "                comments.append(\n",
    "                    {\n",
    "                        \"user_name\": c[\"comment_author\"],\n",
    "                        \"artwork_title\": row[\"title\"],\n",
    "                        \"timestamp\": c[\"comment_date\"],\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LorenzoStancato\\AppData\\Local\\Temp\\ipykernel_5284\\2488800568.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  interactions_df[\"timestamp\"].replace(\"N/A\", \"01/01/1970\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "interactions_df = pd.DataFrame(comments)\n",
    "user_map = {name: idx for idx, name in enumerate(users_df[\"name\"].unique())}\n",
    "artwork_map = {\n",
    "    title: idx + len(user_map)\n",
    "    for idx, title in enumerate(artworks_df[\"title\"].unique())\n",
    "}\n",
    "interactions_df[\"timestamp\"].replace(\"N/A\", \"01/01/1970\", inplace=True)\n",
    "interactions_df[\"timestamp\"] = (\n",
    "    pd.to_datetime(interactions_df[\"timestamp\"], format=\"%d/%m/%Y\")\n",
    ")\n",
    "interactions_df[\"timestamp\"] = interactions_df[\"timestamp\"].astype(\"int64\") // 10**9\n",
    "source_nodes = interactions_df[\"user_name\"].map(user_map).tolist()\n",
    "destination_nodes = interactions_df[\"artwork_title\"].map(artwork_map).tolist()\n",
    "timestamps = interactions_df[\"timestamp\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(user_map) + len(artwork_map)\n",
    "memory_dim = 128  \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_memory = torch.zeros((num_nodes, memory_dim), dtype=torch.float)\n",
    "node_embeddings = torch.zeros((num_nodes, embedding_dim), dtype=torch.float)\n",
    "last_update = torch.zeros(num_nodes, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "attention_weights = torch.nn.Parameter(\n",
    "    torch.randn(num_heads, embedding_dim , embedding_dim )\n",
    ")\n",
    "time_encoding_weights = torch.nn.Parameter(\n",
    "    torch.randn(num_heads, embedding_dim)\n",
    ")\n",
    "\n",
    "\n",
    "def multi_head_attention(source_emb, neighbor_embs, delta_times):\n",
    "    delta_times = delta_times.unsqueeze(1).repeat(1, num_heads, 1)\n",
    "    time_encoding = torch.sin(delta_times.unsqueeze(2) * time_encoding_weights)\n",
    "    attention_scores = []\n",
    "    for head in range(num_heads):\n",
    "        head_attention_scores = source_emb @ attention_weights[head]\n",
    "        head_attention_scores += time_encoding[:, head, head, :].squeeze(1)\n",
    "        attention_scores.append(F.softmax(head_attention_scores, dim=1))\n",
    "    aggregated_embedding = torch.cat(\n",
    "        [\n",
    "            attn * neighbor_embs\n",
    "            for attn in attention_scores\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    return aggregated_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = torch.nn.Parameter(torch.ones(1))\n",
    "def handle_staleness(last_update, current_time, memory):\n",
    "    time_diff = current_time - last_update\n",
    "    decay_factor = torch.exp(-decay_rate * time_diff.unsqueeze(1).float())\n",
    "    decayed_memory = memory * decay_factor\n",
    "    return decayed_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(input_size=memory_dim + embedding_dim, hidden_size=memory_dim)\n",
    "def update_memory_gru(message, memory):\n",
    "    message = message.unsqueeze(0)  \n",
    "    memory = memory.unsqueeze(0)  \n",
    "    updated_memory, _ = gru(message, memory)\n",
    "    return updated_memory.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weights = torch.nn.Parameter(torch.randn(embedding_dim)*0.01)\n",
    "def normalize_time(delta_times):\n",
    "    delta_times = delta_times - delta_times.min()\n",
    "    delta_times = delta_times / (60 * 60 * 24)\n",
    "    delta_times = torch.clamp(delta_times, min=1e-8)\n",
    "    normalized_time = delta_times.unsqueeze(1) * norm_weights\n",
    "    normalized_time = torch.clamp(normalized_time, min=-10, max=10)\n",
    "    return torch.exp(-normalized_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_memory = torch.randn(((memory_dim + embedding_dim) * 2, memory_dim), requires_grad=True)\n",
    "b_memory = torch.zeros(memory_dim, requires_grad=True)\n",
    "\n",
    "W_output = torch.randn((memory_dim, 1), requires_grad=True)\n",
    "b_output = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nodes(nodes):\n",
    "    nodes = [-1 if np.isnan(x) else int(x) for x in nodes]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(\n",
    "    source_nodes,\n",
    "    destination_nodes,\n",
    "    timestamps,\n",
    "    last_update,\n",
    "    node_memory,\n",
    "    current_time,\n",
    "):\n",
    "    valid_indices = [\n",
    "        i for i, s in enumerate(source_nodes) if s != -1 and destination_nodes[i] != -1\n",
    "    ]\n",
    "    if len(valid_indices) == 0:\n",
    "        return node_memory\n",
    "    source_nodes = torch.tensor(\n",
    "        [source_nodes[i] for i in valid_indices], dtype=torch.long\n",
    "    )\n",
    "    destination_nodes = torch.tensor(\n",
    "        [destination_nodes[i] for i in valid_indices], dtype=torch.long\n",
    "    )\n",
    "    timestamps = torch.tensor([timestamps[i] for i in valid_indices], dtype=torch.long)\n",
    "    source_memory = node_memory[source_nodes]\n",
    "    destination_memory = node_memory[destination_nodes]\n",
    "    delta_times = timestamps - last_update[source_nodes]\n",
    "    source_memory = handle_staleness(\n",
    "        last_update[source_nodes], current_time, source_memory\n",
    "    )\n",
    "    normalized_time = normalize_time(delta_times)\n",
    "    aggregated_embedding = multi_head_attention(\n",
    "        source_memory, destination_memory, normalized_time\n",
    "    )\n",
    "    updated_memory = torch.tanh(aggregated_embedding.clone() @ W_memory + b_memory)\n",
    "    node_memory = node_memory.clone()\n",
    "    node_memory[source_nodes] = updated_memory\n",
    "\n",
    "    return node_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x24a511813d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        W_memory,\n",
    "        b_memory,\n",
    "        W_output,\n",
    "        b_output,\n",
    "        decay_rate,\n",
    "        norm_weights,\n",
    "        attention_weights,\n",
    "        time_encoding_weights,\n",
    "    ],\n",
    "    lr=0.001,\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:768: UserWarning: Error detected in IndexPutBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\LorenzoStancato\\AppData\\Local\\Temp\\ipykernel_5284\\1332283952.py\", line 14, in <module>\n",
      "    node_memory = process_batch(\n",
      "  File \"C:\\Users\\LorenzoStancato\\AppData\\Local\\Temp\\ipykernel_5284\\2858706213.py\", line 33, in process_batch\n",
      "    node_memory[source_nodes] = updated_memory\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:116.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(score\u001b[38;5;241m.\u001b[39msqueeze(), label)\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    current_time = timestamps[-1] \n",
    "\n",
    "    for i in range(0, len(source_nodes), batch_size):\n",
    "        backward_called = False\n",
    "        batch_source = source_nodes[i : i + batch_size]\n",
    "        batch_dest = destination_nodes[i : i + batch_size]\n",
    "        batch_source =clean_nodes(batch_source)\n",
    "        batch_dest = clean_nodes(batch_dest)\n",
    "        batch_times = timestamps[i : i + batch_size]\n",
    "        node_memory = process_batch(\n",
    "            batch_source,\n",
    "            batch_dest,\n",
    "            batch_times,\n",
    "            last_update,\n",
    "            node_memory,\n",
    "            current_time,\n",
    "        )\n",
    "        if torch.isnan(node_memory).any():\n",
    "            node_memory = torch.nan_to_num(node_memory, nan=0.0)\n",
    "        filtered_batch_source = [s for s in batch_source if s != -1]\n",
    "        filtered_batch_dest = [d for d in batch_dest if d != -1]\n",
    "        src_emb = node_memory[filtered_batch_source]\n",
    "        dst_emb = node_memory[filtered_batch_dest]\n",
    "        if src_emb.shape[0] < 32:\n",
    "            padding = torch.zeros(32 - src_emb.shape[0], 128)  \n",
    "            src_emb = torch.cat([src_emb, padding], dim=0)\n",
    "            dst_emb = torch.cat([dst_emb, padding], dim=0)\n",
    "        dst_emb= dst_emb[:32,:]\n",
    "        score = (src_emb * dst_emb) @ W_output + b_output\n",
    "        label = torch.ones(batch_size)\n",
    "        loss = F.binary_cross_entropy_with_logits(score.squeeze(), label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(source_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_artworks(user_name, top_k=5):\n",
    "    user_idx = user_map.get(user_name)\n",
    "    if user_idx is None:\n",
    "        print(\"Utente non trovato.\")\n",
    "        return []\n",
    "\n",
    "    user_memory = node_memory[user_idx]\n",
    "    artwork_indices = torch.arange(len(user_map), num_nodes)\n",
    "    user_emb = user_memory.unsqueeze(0)\n",
    "    artworks_memory = node_memory[artwork_indices]\n",
    "    scores = (user_emb * artworks_memory).sum(dim=1) @ W_output + b_output\n",
    "    top_scores, top_indices = torch.topk(scores, k=top_k)\n",
    "    recommended_artworks = [\n",
    "        list(artwork_map.keys())[i - len(user_map)] for i in top_indices.tolist()\n",
    "    ]\n",
    "\n",
    "    return recommended_artworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_recommend = \"exarobibliologist\" \n",
    "recommendations = recommend_artworks(user_to_recommend)\n",
    "print(f\"Raccomandazioni per {user_to_recommend}: {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
