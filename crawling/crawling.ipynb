{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException , WebDriverException\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_images(driver, by, value, timeout=10):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_all_elements_located((by, value))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(likes_str):\n",
    "    if \"K\" in likes_str:\n",
    "        return int(float(likes_str.replace(\"K\", \"\")) * 1000)\n",
    "    else:\n",
    "        return int(likes_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date_str):\n",
    "    match_now = re.match(r\"just now\", date_str)\n",
    "    match_yesterday = re.match(r\"Yesterday\", date_str)\n",
    "    match_days = re.match(r\"(\\d+) days ago\", date_str)\n",
    "    match_minutes = re.match(r\"(\\d+) mins? ago\", date_str)\n",
    "    match_hours = re.match(r\"(\\d+) hours? ago\", date_str)\n",
    "    if match_now:\n",
    "        date_obj = datetime.now()\n",
    "    elif match_minutes:\n",
    "        minutes_ago = int(match_minutes.group(1))\n",
    "        date_obj = datetime.now() - timedelta(minutes=minutes_ago)\n",
    "    elif match_hours:\n",
    "        hours_ago = int(match_hours.group(1))\n",
    "        date_obj = datetime.now() - timedelta(hours=hours_ago)\n",
    "    elif match_yesterday:\n",
    "        date_obj = datetime.now() - timedelta(days=1)\n",
    "    elif match_days:\n",
    "        days_ago = int(match_days.group(1))\n",
    "        date_obj = datetime.now() - timedelta(days=days_ago)\n",
    "    else:\n",
    "        date_obj = datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "        \n",
    "    formatted_date = date_obj.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_more_comments(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button._1lBsK._3_MJY._2vim0.ds-card._1FKeR\"))\n",
    "            )\n",
    "            load_more_button.click()\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.invisibility_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"div.lxZkf._2ywna\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_more_content(driver, section_id):\n",
    "    while True:\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        f\"//section[@id='{section_id}']//button[contains(., 'Load more')]\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            driver.execute_script(\n",
    "                \"arguments[0].scrollIntoView(true);\", load_more_button\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_next_page(driver, next_page_number,site_url, scroll_pause_time=5):\n",
    "    driver.get(site_url)\n",
    "    for i in range(next_page_number):        \n",
    "        try:\n",
    "            next_page_buttons = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH, \"//a[contains(@class, '_1OGeq')]\")\n",
    "                )\n",
    "            )\n",
    "            if len(next_page_buttons) == 0:\n",
    "                print(\"Nessun elemento trovato.\")\n",
    "            if len(next_page_buttons) >= 2:\n",
    "                next_page_button = next_page_buttons[1]\n",
    "            else:\n",
    "                next_page_button = next_page_buttons[0]\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_page_button)\n",
    "            time.sleep(scroll_pause_time)\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page_button)\n",
    "        except TimeoutException:\n",
    "                print(\"L'elemento non Ã¨ stato trovato entro il tempo specificato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number(string):\n",
    "    number = re.findall(r\"\\d+\", string)\n",
    "    if number:\n",
    "        return int(number[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_date(date):\n",
    "    year = find_number(date)\n",
    "    if year:\n",
    "        int_year = int(year)\n",
    "        current_date = datetime.now()\n",
    "        inscription_date = current_date.replace(year= current_date.year - int_year)\n",
    "        return inscription_date.strftime(\"%d/%m/%Y\")\n",
    "    else:\n",
    "        return 'N/A'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user_date_place(tags):\n",
    "    \n",
    "    inscription_date = 'N/A'\n",
    "    place= 'N/A'\n",
    "    for tag in tags:\n",
    "        text= tag.text\n",
    "        if 'Deviant for' in text:\n",
    "            inscription_date= user_date(text)\n",
    "    \n",
    "    if tags[1] and tags[1].text.isalpha():\n",
    "        place= tags[1].text\n",
    "    return inscription_date, place\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_string(s, chunk_size):\n",
    "    return [s[i:i+chunk_size] for i in range(0, len(s), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_user(artwork, driver):   \n",
    "    try:\n",
    "\n",
    "        detail_url= artwork.find('a')['href']\n",
    "        driver.get(detail_url)\n",
    "        wait_for_images(driver, By.CLASS_NAME, \"_23oWS\")\n",
    "        detail_content = driver.page_source\n",
    "        detail_soup = BeautifulSoup(detail_content, \"html.parser\")\n",
    "\n",
    "        user_link_container = detail_soup.find(\"div\", class_=\"_31DYV\")\n",
    "        user_link_tag = user_link_container.find(\"a\", class_=\"user-link _277bf\")\n",
    "        user_link = user_link_tag['href'] if user_link_tag else 'N/A'\n",
    "        replaced_user_link = user_link.replace('/gallery', '/about')\n",
    "\n",
    "        driver.get(replaced_user_link)\n",
    "        wait_for_images(driver, By.CLASS_NAME, \"_2UI2c\")\n",
    "        user_content = driver.page_source\n",
    "        user_soup = BeautifulSoup(user_content, \"html.parser\")\n",
    "\n",
    "        name_tag = user_soup.find(\"span\", class_=\"_2UI2c\")\n",
    "        name = name_tag.text if name_tag else 'N/A'\n",
    "        date_place_tags= user_soup.find_all(\"span\", class_=\"_2cHeo\")\n",
    "        inscription_date, place= find_user_date_place(date_place_tags) if date_place_tags else ('N/A', 'N/A')\n",
    "        statistics_tags = user_soup.find_all(\"div\", class_=\"_2C2cW\")\n",
    "\n",
    "        page_views= 'N/A'\n",
    "        num_followers= 'N/A'\n",
    "        num_follow= 'N/A'\n",
    "        num_favourites= 'N/A'\n",
    "        num_comments_made= 'N/A'\n",
    "        num_comments_received= 'N/A'\n",
    "\n",
    "        if len(statistics_tags) != 0:\n",
    "            page_views = convert_text(statistics_tags[0].find(string=True, recursive=False).strip()) if statistics_tags[0] else 'N/A'\n",
    "            num_followers = convert_text(statistics_tags[2].find(string=True, recursive=False).strip()) if statistics_tags[2] else 'N/A'\n",
    "            num_follow = convert_text(statistics_tags[3].find(string=True, recursive=False).strip()) if statistics_tags[3] else 'N/A'\n",
    "            num_favourites = convert_text(statistics_tags[4].find(string=True, recursive=False).strip()) if statistics_tags[4] else 'N/A'\n",
    "            num_comments_made= convert_text(statistics_tags[5].find(string=True, recursive=False).strip()) if statistics_tags[5] else 'N/A'\n",
    "            num_comments_received= convert_text(statistics_tags[6].find(string=True, recursive=False).strip()) if statistics_tags[6] else 'N/A'\n",
    "\n",
    "        load_more_content(driver, 'watchers')\n",
    "        load_more_content(driver, 'watching')\n",
    "        user_content = driver.page_source\n",
    "        user_soup = BeautifulSoup(user_content, \"html.parser\")\n",
    "\n",
    "        followers= []\n",
    "        watchers_section = user_soup.find(\"section\", id=\"watchers\")\n",
    "        followers_tags = watchers_section.find_all(\"div\", class_=\"_2YmxY\") if watchers_section else []\n",
    "\n",
    "        for tag in followers_tags:\n",
    "            follower_name_tag = tag.find(\"span\", class_=\"_2QMci\")\n",
    "            follower_name = follower_name_tag.text if follower_name_tag else 'N/A'\n",
    "            if follower_name != \"N/A\":\n",
    "                followers.append({\n",
    "                    \"follower\": follower_name,\n",
    "                })\n",
    "        follow = []\n",
    "        watching_section = user_soup.find(\"section\", id=\"watching\")\n",
    "        watching_tags = watching_section.find_all(\"div\", class_=\"_2YmxY\") if watching_section else []\n",
    "\n",
    "        for tag in watching_tags:\n",
    "            watching_name_tag = tag.find(\"span\", class_=\"_2QMci\")\n",
    "            watching_name = watching_name_tag.text if watching_name_tag else \"N/A\"\n",
    "            if watching_name != \"N/A\":\n",
    "                follow.append(\n",
    "                    {\n",
    "                        \"follow\": watching_name,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        user_object = {\n",
    "                \"name\": name,\n",
    "                \"place\": place,\n",
    "                \"inscription_date\": inscription_date,\n",
    "                \"page_views\": page_views,\n",
    "                \"num_followers\": num_followers,\n",
    "                \"num_follow\": num_follow,\n",
    "                \"num_favourites\": num_favourites,\n",
    "                \"num_comments_made\": num_comments_made,\n",
    "                \"num_comments_received\": num_comments_received,\n",
    "                \"followers\": followers,\n",
    "                \"follow\": follow\n",
    "            }\n",
    "        return user_object\n",
    "    except Exception as e:\n",
    "        print(f\"Errore in user: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_artworks(artwork, driver):\n",
    "        try:\n",
    "            img_tag= artwork.find('img')\n",
    "            img_url = img_tag['src'] if img_tag else 'N/A'\n",
    "\n",
    "            title_tag = artwork.find('h2', class_=\"_3CpJS\")\n",
    "            title= title_tag.text if title_tag else 'N/A'\n",
    "\n",
    "            author_tag = artwork.find(\"span\", class_=\"_2EfV7\")\n",
    "            author = author_tag.text if author_tag else 'N/A'\n",
    "\n",
    "            likes_tag = artwork.find('button', class_='reset-button _2Pdhv')\n",
    "            likes_span = likes_tag.find('span', class_='_20Nv2').find_next_sibling('span') if likes_tag else 'N/A'\n",
    "            likes = convert_text(likes_span.text) if likes_span else \"N/A\"\n",
    "\n",
    "            comments_tag = artwork.find('a', href=lambda href: href and href.endswith('#comments'))\n",
    "            comments_span = comments_tag.find('span').next_sibling if comments_tag else 'N/A'\n",
    "            comments = convert_text(comments_span.text) if comments_span else \"N/A\"\n",
    "\n",
    "            detail_url= artwork.find('a')['href']\n",
    "            driver.get(detail_url)\n",
    "            wait_for_images(driver, By.CLASS_NAME, \"_23oWS\")\n",
    "\n",
    "            load_more_comments(driver)\n",
    "            detail_content = driver.page_source\n",
    "            detail_soup = BeautifulSoup(detail_content, \"html.parser\")\n",
    "\n",
    "            time_tag = detail_soup.find('time')\n",
    "            time = time_tag.text if time_tag else 'N/A'\n",
    "            time = format_date(time)\n",
    "\n",
    "            views_span = detail_soup.find(\"span\", class_=\"reset-button _3bGQn _1otI6\").find(\n",
    "                \"span\", class_=\"_3AClx\"\n",
    "            ).find('span')\n",
    "            views = convert_text(views_span.text) if views_span else \"N/A\"\n",
    "\n",
    "            image_tags =[]\n",
    "            tag_container = detail_soup.find(\"div\", class_=\"aodIv wT4l_\")\n",
    "            tags = tag_container.find_all(\"a\")if tag_container else []\n",
    "            for tag in tags:\n",
    "                image_tags.append(tag.text)\n",
    "\n",
    "            image_comments = []\n",
    "            comments_container = detail_soup.find(\"div\", class_=\"_1YhYy\")\n",
    "            all_comments = (\n",
    "                comments_container.find_all(\"div\", class_=\"_2VfPz _1LomQ\")\n",
    "                if comments_container\n",
    "                else []\n",
    "            )\n",
    "\n",
    "            for comment in all_comments:\n",
    "                comment_author_container = comment.find(\"span\", class_=\"_2vKEO\")\n",
    "                comment_author = comment_author_container.text if comment_author_container else \"N/A\"\n",
    "                comment_date = comment.find(\"time\")\n",
    "                formatted_comment_date = format_date(comment_date.text) if comment_date else \"N/A\"\n",
    "                comment_text_container = comment.find(\n",
    "                    \"span\", class_=\"_2PHJq public-DraftStyleDefault-ltr\"\n",
    "                )\n",
    "                comment_text= comment_text_container.text if comment_text_container else \"N/A\"\n",
    "                image_comments.append(\n",
    "                    {\n",
    "                        \"comment_author\": comment_author,\n",
    "                        \"comment_date\": formatted_comment_date,\n",
    "                        \"comment_text\": comment_text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            artwork_object = {\n",
    "                \"img\": img_url,\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"likes\": likes,\n",
    "                \"number_of_comments\": comments,\n",
    "                \"date\": time,\n",
    "                'number_of_views': views,\n",
    "                'image_tags': image_tags,\n",
    "                'comments': image_comments\n",
    "            }\n",
    "            return artwork_object\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for artwork: {e}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_url = \"https://www.deviantart.com\"\n",
    "options = ChromeOptions()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(site_url)\n",
    "\n",
    "wait_for_images(driver, By.CLASS_NAME, \"_3_LJY\")\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "artworks = soup.findAll(\"div\", class_=\"_3Y0hT _3oBlM\")\n",
    "filtered_artworks = [\n",
    "    artwork for artwork in artworks if not artwork.find(\"span\", class_=\"_10VHP\")\n",
    "]\n",
    "\n",
    "artworks_data = []\n",
    "user_data=[]\n",
    "i=0\n",
    "while i <20:\n",
    "    try:\n",
    "        wait_for_images(driver, By.CLASS_NAME, \"_3_LJY\")\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        artworks = soup.findAll(\"div\", class_=\"_3Y0hT _3oBlM\")\n",
    "        filtered_artworks = [\n",
    "            artwork for artwork in artworks if not artwork.find(\"span\", class_=\"_10VHP\")\n",
    "        ]\n",
    "        for artwork in filtered_artworks:\n",
    "            artworks_data.append(crawling_artworks(artwork, driver))\n",
    "            user_data.append(crawling_user(artwork, driver))\n",
    "        print(len(artworks_data))\n",
    "        print(len(user_data))\n",
    "        go_to_next_page(driver, i+1, site_url)\n",
    "        i+=1\n",
    "    except WebDriverException as e:\n",
    "        print(f\"Errore in crawling: {e}\")\n",
    "        time.sleep(5)\n",
    "        driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size= 30000\n",
    "max_comments_chunk = max(\n",
    "    len(chunk_string(json.dumps(artwork[\"comments\"], ensure_ascii=False), chunk_size))\n",
    "    for artwork in artworks_data\n",
    "    if artwork is not None\n",
    ")\n",
    "header = [\n",
    "    \"img\",\n",
    "    \"title\",\n",
    "    \"author\",\n",
    "    \"likes\",\n",
    "    \"number_of_comments\",\n",
    "    \"number_of_views\",\n",
    "    \"date\",\n",
    "    \"image_tags\",\n",
    "] + [f\"comment_{i+1}\" for i in range(max_comments_chunk)]\n",
    "with open('../dataset/artworks.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    writer.writerow(header)      \n",
    "\n",
    "    for artwork in artworks_data:\n",
    "        if artwork is not None:\n",
    "            comments_chunk = chunk_string(\n",
    "                json.dumps(artwork[\"comments\"], ensure_ascii=False), chunk_size\n",
    "            )\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    artwork[\"img\"],\n",
    "                    artwork[\"title\"],\n",
    "                    artwork[\"author\"],\n",
    "                    artwork[\"likes\"],\n",
    "                    artwork[\"number_of_comments\"],\n",
    "                    artwork[\"number_of_views\"],\n",
    "                    artwork[\"date\"],\n",
    "                    artwork[\"image_tags\"],\n",
    "                ]\n",
    "                + comments_chunk\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 30000\n",
    "max_followers_chunks = max(len(chunk_string(json.dumps(user[\"followers\"], ensure_ascii=False), chunk_size)) for user in user_data if user is not None)\n",
    "max_follow_chunks = max(len(chunk_string(json.dumps(user[\"follow\"], ensure_ascii=False), chunk_size)) for user in user_data if user is not None)\n",
    "header = (\n",
    "    [\n",
    "        \"name\",\n",
    "        \"place\",\n",
    "        \"inscription_date\",\n",
    "        \"number_page_views\",\n",
    "        \"number_followers\",\n",
    "        \"number_follow\",\n",
    "        \"number_favourites\",\n",
    "        \"number_comments_made\",\n",
    "        \"number_comments_receveid\",\n",
    "    ]\n",
    "    + [f\"followers_part_{i+1}\" for i in range(max_followers_chunks)]\n",
    "    + [f\"follow_part_{i+1}\" for i in range(max_follow_chunks)]\n",
    ")\n",
    "with open(\"../dataset/users.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "    writer.writerow(header)\n",
    "    for user in user_data:\n",
    "        if user is not None:\n",
    "            followers_json = json.dumps(user[\"followers\"], ensure_ascii=False)\n",
    "            follow_json = json.dumps(user[\"follow\"], ensure_ascii=False)\n",
    "            followers_chunk = chunk_string(followers_json, chunk_size)\n",
    "            follow_chunk = chunk_string(follow_json, chunk_size)\n",
    "            row=[\n",
    "                    user[\"name\"],\n",
    "                    user[\"place\"],\n",
    "                    user[\"inscription_date\"],\n",
    "                    user[\"page_views\"],\n",
    "                    user[\"num_followers\"],\n",
    "                    user[\"num_follow\"],\n",
    "                    user[\"num_favourites\"],\n",
    "                    user[\"num_comments_made\"],\n",
    "                    user[\"num_comments_received\"],\n",
    "                ]\n",
    "            for i in range(max_followers_chunks):\n",
    "                if i < len(followers_chunk):\n",
    "                    row.append(followers_chunk[i])\n",
    "                else:\n",
    "                    row.append(\"\")\n",
    "            for i in range(max_follow_chunks):\n",
    "                if i < len(follow_chunk):\n",
    "                    row.append(follow_chunk[i])\n",
    "                else:\n",
    "                    row.append(\"\")\n",
    "            writer.writerow(row)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
