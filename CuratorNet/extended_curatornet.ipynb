{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h1QplZJxzqm_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Concatenate,\n",
        "    Input,\n",
        "    Embedding,\n",
        "    Lambda,\n",
        "    TextVectorization,\n",
        "    Normalization,\n",
        "    GlobalAveragePooling2D,\n",
        "    GlobalAveragePooling1D,\n",
        ")\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7279pOar0b98"
      },
      "outputs": [],
      "source": [
        "user_df = pd.read_csv(\"../dataset/users.csv\", delimiter=\";\")\n",
        "artwork_df = pd.read_csv(\"../dataset/artworks.csv\", delimiter=\";\")\n",
        "artwork_df.fillna(-1, inplace=True)\n",
        "user_df.fillna(-1, inplace=True)\n",
        "artworks = artwork_df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
        "users = user_df.drop_duplicates(subset=\"name\", keep=\"first\")\n",
        "artwork_df[\"tag_string\"] = artwork_df[\"image_tags\"].apply(\n",
        "    lambda x: \" \".join(ast.literal_eval(x))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tag_vectors = tfidf_vectorizer.fit_transform(artwork_df[\"tag_string\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hr1n5IXp9Lmj"
      },
      "outputs": [],
      "source": [
        "user_df[\"place\"] = user_df[\"place\"].astype(str)\n",
        "user_df[\"place\"] = user_df[\"place\"].fillna(\"N/A\")\n",
        "\n",
        "user_df[\"inscription_date\"] = user_df[\"inscription_date\"].astype(str)\n",
        "user_df[\"inscription_date\"] = user_df[\"inscription_date\"].fillna(\"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DD6iZ2370dcV"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_url):\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = image.resize((224, 224))\n",
        "        image_array = np.array(image)\n",
        "        image_array = image_array / 255.0\n",
        "        return image_array\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Impossibile scaricare l'immagine dall'URL: {image_url}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_similar_image(user_artworks, artwork_df, tag_vectors):\n",
        "    user_tags_string = \" \".join(user_artworks[\"tag_string\"].values)\n",
        "    user_tags_vector = tfidf_vectorizer.transform([user_tags_string])\n",
        "    similarity = cosine_similarity(user_tags_vector, tag_vectors)\n",
        "    average_similarity = similarity.mean(axis=0)\n",
        "    similar_image_index = average_similarity.argmax()\n",
        "    return pd.DataFrame([artwork_df.iloc[similar_image_index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wFV80hZI0ekm"
      },
      "outputs": [],
      "source": [
        "def create_triples(user_df, artwork_df):\n",
        "    triples = []\n",
        "    for _, user_row in user_df.iterrows():\n",
        "        user_interactions = artwork_df[artwork_df[\"author\"] == user_row[\"name\"]]\n",
        "        Pu = user_interactions.sample(frac=1)\n",
        "        i = get_similar_image(user_interactions, artwork_df, tag_vectors)\n",
        "        non_interacted = artwork_df[~artwork_df[\"author\"].isin([user_row[\"name\"]])]\n",
        "        j = non_interacted.sample()\n",
        "        triples.append((Pu, i, j))\n",
        "    return triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GuQsFuBI0gSE"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(triples, image_preprocessor):\n",
        "    processed_triples = []\n",
        "\n",
        "    for Pu, i, j in triples:\n",
        "        Pu_images = np.array([image_preprocessor(url) for url in Pu[\"img\"].values])\n",
        "        i_image = np.array(image_preprocessor(i[\"img\"].values[0]))\n",
        "        j_image = np.array(image_preprocessor(j[\"img\"].values[0]))\n",
        "        processed_triples.append((Pu_images, i_image, j_image, Pu, i.iloc[0], j.iloc[0]))\n",
        "\n",
        "    return processed_triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HLNbF3dO0GM3"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200\n",
        "pu_dim = 400\n",
        "max_text_words = 5000\n",
        "max_comment_words = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tr8OwgQR67O_"
      },
      "outputs": [],
      "source": [
        "name_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "name_vectorizer.adapt(user_df[\"name\"])\n",
        "place_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "place_vectorizer.adapt(user_df[\"place\"])\n",
        "\n",
        "inscription_date_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "inscription_date_vectorizer.adapt(user_df[\"inscription_date\"])\n",
        "\n",
        "art_title_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_title_vectorizer.adapt(artwork_df[\"title\"])\n",
        "\n",
        "art_author_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_author_vectorizer.adapt(artwork_df[\"author\"])\n",
        "\n",
        "art_date_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_date_vectorizer.adapt(artwork_df[\"date\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Ms3tYKp0Hut"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling=None)\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "def extract_features(image):\n",
        "    features=resnet(image)\n",
        "    features = GlobalAveragePooling2D()(features)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8bQTxOgk0JT1"
      },
      "outputs": [],
      "source": [
        "def create_text_embedding(text_input, max_words, output_dim):\n",
        "    embedding = Embedding(input_dim=max_words, output_dim=output_dim)(text_input)\n",
        "    pooling_embedding = GlobalAveragePooling1D()(embedding)\n",
        "    return pooling_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8aUYCDfy0LV9"
      },
      "outputs": [],
      "source": [
        "normalization_layer = Normalization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q_zSPG40Njl",
        "outputId": "fa22ea0c-eb34-4ff8-9939-fb0f6661901f"
      },
      "outputs": [],
      "source": [
        "input_user_name = Input(shape=(1,), name=\"input_user_name\")\n",
        "input_user_place = Input(shape=(1,), name=\"input_user_place\")\n",
        "input_user_inscription_date = Input(shape=(1,), name=\"input_user_inscription_date\")\n",
        "input_user_page_views = Input(shape=(1,), name=\"input_user_page_views\")\n",
        "input_user_followers = Input(shape=(1,), name=\"input_user_followers\")\n",
        "input_user_follow = Input(shape=(1,), name=\"input_user_follow\")\n",
        "input_user_favourites = Input(shape=(1,), name=\"input_user_favourites\")\n",
        "input_user_comments_made = Input(shape=(1,), name=\"input_user_comments_made\")\n",
        "input_user_comments_received = Input(shape=(1,), name=\"input_user_comments_received\")\n",
        "\n",
        "normalized_inscription_date = normalization_layer(input_user_inscription_date)\n",
        "normalized_page_views = normalization_layer(input_user_page_views)\n",
        "normalized_followers = normalization_layer(input_user_followers)\n",
        "normalized_follow = normalization_layer(input_user_follow)\n",
        "normalized_favourites = normalization_layer(input_user_favourites)\n",
        "normalized_comments_made = normalization_layer(input_user_comments_made)\n",
        "normalized_comments_received = normalization_layer(input_user_comments_received)\n",
        "\n",
        "normalized_inscription_date = Dense(embedding_dim, activation='selu')(normalized_inscription_date)\n",
        "normalized_page_views = Dense(embedding_dim, activation=\"selu\")(normalized_page_views)\n",
        "normalized_followers = Dense(embedding_dim, activation=\"selu\")(normalized_followers)\n",
        "normalized_follow = Dense(embedding_dim, activation=\"selu\")(normalized_follow)\n",
        "normalized_favourites = Dense(embedding_dim, activation=\"selu\")(normalized_favourites)\n",
        "normalized_comments_made = Dense(embedding_dim, activation=\"selu\")(\n",
        "    normalized_comments_made\n",
        ")\n",
        "normalized_comments_received = Dense(embedding_dim, activation=\"selu\")(\n",
        "    normalized_comments_received\n",
        ")\n",
        "\n",
        "user_features = Concatenate(name=\"user_features\")(\n",
        "    [\n",
        "        create_text_embedding(input_user_name, max_text_words, embedding_dim),\n",
        "        create_text_embedding(input_user_place, max_text_words, embedding_dim),\n",
        "        create_text_embedding(input_user_inscription_date, max_text_words, embedding_dim),\n",
        "        normalized_page_views,\n",
        "        normalized_followers,\n",
        "        normalized_follow,\n",
        "        normalized_favourites,\n",
        "        normalized_comments_made,\n",
        "        normalized_comments_received,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6zedcZS40OUT"
      },
      "outputs": [],
      "source": [
        "input_art_title = Input(shape=(14,), name=\"input_art_title\")\n",
        "input_art_author = Input(shape=(1,), name=\"input_art_author\")\n",
        "input_art_likes = Input(shape=(1,), name=\"input_art_likes\")\n",
        "input_art_views = Input(shape=(1,), name=\"input_art_views\")\n",
        "input_art_date = Input(shape=(1,), name=\"input_art_date\")\n",
        "\n",
        "art_title_embedding = create_text_embedding(\n",
        "    input_art_title, max_text_words, embedding_dim\n",
        ")\n",
        "art_author_embedding = create_text_embedding(\n",
        "    input_art_author, max_text_words, embedding_dim\n",
        ")\n",
        "art_date_embedding = create_text_embedding(\n",
        "    input_art_date, max_text_words, embedding_dim\n",
        ")\n",
        "\n",
        "art_features = Concatenate(name=\"art_features\")(\n",
        "    [\n",
        "        art_title_embedding,\n",
        "        art_author_embedding,\n",
        "        normalization_layer(input_art_likes),\n",
        "        normalization_layer(input_art_views),\n",
        "        normalization_layer(input_art_date),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1hxgbdqY0QW5"
      },
      "outputs": [],
      "source": [
        "def custom_reduce_sum(x, y):\n",
        "    return K.sum(x * y, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h-YEp9Az0SGd"
      },
      "outputs": [],
      "source": [
        "def custom_triplet_loss(y_true, y_pred, margin=0.4):\n",
        "    score_i, score_j = tf.split(y_pred, num_or_size_splits=2, axis=-1)\n",
        "    loss = tf.maximum(0.0, margin + score_j - score_i)\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l10MY_k0UoP",
        "outputId": "1153efb9-195b-4b78-d68d-39c6e0065dd1"
      },
      "outputs": [],
      "source": [
        "input_pu = Input(shape=(3,224, 224, 3), name=\"input_pu\")\n",
        "input_i = Input(shape=(224, 224, 3), name=\"input_i\")\n",
        "input_j = Input(shape=(224, 224, 3), name=\"input_j\")\n",
        "pu_features = Lambda(\n",
        "    lambda x: K.map_fn(\n",
        "        lambda y: extract_features(y), x\n",
        "    ),\n",
        "    output_shape=(None, embedding_dim),\n",
        ")(input_pu)\n",
        "\n",
        "i_features = extract_features(input_i)\n",
        "j_features = extract_features(input_j)\n",
        "\n",
        "dense_layer_1 = Dense(embedding_dim, activation=\"selu\", name=\"dense_layer_1\")\n",
        "dense_layer_2 = Dense(embedding_dim, activation=\"selu\", name=\"dense_layer_2\")\n",
        "\n",
        "reduced_pu = Lambda(\n",
        "    lambda x: K.map_fn(\n",
        "        lambda y: dense_layer_2(dense_layer_1(y)), x\n",
        "    ),\n",
        "    output_shape=(None, embedding_dim),\n",
        ")(pu_features)\n",
        "\n",
        "\n",
        "reduced_i = dense_layer_2(dense_layer_1(i_features))\n",
        "reduced_j = dense_layer_2(dense_layer_1(j_features))\n",
        "\n",
        "reduced_user_features = Dense(300, activation=\"relu\", name=\"user_embedding\")(\n",
        "    user_features\n",
        ")\n",
        "reduced_art_features = Dense(300, activation=\"relu\", name=\"artwork_embedding\")(\n",
        "    art_features\n",
        ")\n",
        "\n",
        "concat_i = Concatenate()([reduced_i, reduced_user_features, reduced_art_features])\n",
        "concat_j = Concatenate()([reduced_j, reduced_user_features, reduced_art_features])\n",
        "\n",
        "\n",
        "dense_comb_i= Dense(embedding_dim, activation=\"selu\", name=\"dense_comb_i\")(concat_i)\n",
        "dense_comb_j= Dense(embedding_dim, activation=\"selu\", name=\"dense_comb_j\")(concat_j)\n",
        "\n",
        "\n",
        "average_pooled_pu = Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,))(\n",
        "    reduced_pu\n",
        ")\n",
        "max_pooled_pu = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim,))(\n",
        "    reduced_pu\n",
        ")\n",
        "\n",
        "pooled_pu = Concatenate()([average_pooled_pu, max_pooled_pu])\n",
        "\n",
        "\n",
        "pu_dense_1 = Dense(300, activation=\"selu\", name=\"pu_dense_1\")(pooled_pu)\n",
        "pu_dense_2 = Dense(200, activation=\"selu\", name=\"pu_dense_2\")(pu_dense_1)\n",
        "\n",
        "final_pu = Dense(200, activation=\"selu\", name=\"pu_dense_3\")(pu_dense_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hp0FRbmn0VXI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "score_i = Lambda(lambda x: K.sum(x[0] * x[1], axis=1, keepdims=True))([final_pu, dense_comb_i])\n",
        "score_j = Lambda(lambda x: K.sum(x[0] * x[1], axis=1, keepdims=True))([final_pu, dense_comb_j])\n",
        "output_scores = Concatenate(axis=-1)([score_i, score_j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pSJwfxqG0YOr",
        "outputId": "7603ba90-97f3-4a2f-9402-bfa379d3c3c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_curatornet_model():\n",
        "    tf.keras.backend.clear_session()\n",
        "    curatornet = Model(\n",
        "        inputs=[\n",
        "            input_user_name,\n",
        "            input_user_place,\n",
        "            input_user_inscription_date,\n",
        "            input_user_page_views,\n",
        "            input_user_followers,\n",
        "            input_user_follow,\n",
        "            input_user_favourites,\n",
        "            input_user_comments_made,\n",
        "            input_user_comments_received,\n",
        "            input_art_title,\n",
        "            input_art_author,\n",
        "            input_art_likes,\n",
        "            input_art_views,\n",
        "            input_art_date,\n",
        "            input_pu,\n",
        "            input_i,\n",
        "            input_j,\n",
        "        ],\n",
        "        outputs=output_scores,\n",
        "    )\n",
        "    return curatornet\n",
        "\n",
        "# curatornet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.00001), loss=custom_triplet_loss)\n",
        "# curatornet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5jXqMqPM0ilM"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs(processed_triples, users_df, target_image_count=3, expected_length=379):\n",
        "    inputs = {\n",
        "        \"input_user_name\": [],\n",
        "        \"input_user_place\": [],\n",
        "        \"input_user_inscription_date\": [],\n",
        "        \"input_user_page_views\": [],\n",
        "        \"input_user_followers\": [],\n",
        "        \"input_user_follow\": [],\n",
        "        \"input_user_favourites\": [],\n",
        "        \"input_user_comments_made\": [],\n",
        "        \"input_user_comments_received\": [],\n",
        "        \"input_art_title\": [],\n",
        "        \"input_art_author\": [],\n",
        "        \"input_art_likes\": [],\n",
        "        \"input_art_views\": [],\n",
        "        \"input_art_date\": [],\n",
        "        \"input_pu\": [],\n",
        "        \"input_i\": [],\n",
        "        \"input_j\": [],\n",
        "    }\n",
        "\n",
        "    for pu_images, i_image, j_image, Pu_meta, i_meta, j_meta in processed_triples:\n",
        "        user_features = users_df[users_df[\"name\"] == i_meta[\"author\"]].iloc[0]\n",
        "        if pu_images.shape[0] < target_image_count:\n",
        "            pad_width = target_image_count - pu_images.shape[0]\n",
        "            pu_images = np.pad(pu_images, ((0, pad_width), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
        "        elif pu_images.shape[0] > target_image_count:\n",
        "            pu_images = pu_images[:target_image_count]\n",
        "\n",
        "        inputs[\"input_pu\"].append(pu_images)\n",
        "        inputs[\"input_i\"].append(i_image)\n",
        "        inputs[\"input_j\"].append(j_image)\n",
        "        inputs[\"input_user_name\"].append(user_features[\"name\"])\n",
        "        inputs[\"input_user_place\"].append(user_features[\"place\"])\n",
        "        inputs[\"input_user_inscription_date\"].append(user_features[\"inscription_date\"])\n",
        "        inputs[\"input_user_page_views\"].append(user_features[\"number_page_views\"])\n",
        "        inputs[\"input_user_followers\"].append(user_features[\"number_followers\"])\n",
        "        inputs[\"input_user_follow\"].append(user_features[\"number_follow\"])\n",
        "        inputs[\"input_user_favourites\"].append(user_features[\"number_favourites\"])\n",
        "        inputs[\"input_user_comments_made\"].append(user_features[\"number_comments_made\"])\n",
        "        inputs[\"input_user_comments_received\"].append(\n",
        "            user_features[\"number_comments_receveid\"]\n",
        "        )\n",
        "        inputs[\"input_art_title\"].append(i_meta[\"title\"])\n",
        "        inputs[\"input_art_author\"].append(i_meta[\"author\"])\n",
        "        inputs[\"input_art_likes\"].append(i_meta[\"likes\"])\n",
        "        inputs[\"input_art_views\"].append(i_meta[\"number_of_views\"])\n",
        "        inputs[\"input_art_date\"].append(i_meta[\"date\"])\n",
        "\n",
        "    for key in inputs:\n",
        "        try:\n",
        "            if not isinstance(inputs[key], np.ndarray):\n",
        "                inputs[key] = np.array(inputs[key])\n",
        "\n",
        "            current_length = len(inputs[key])\n",
        "            if current_length != expected_length:\n",
        "                print(f\"Adjusting {key} from {current_length} to {expected_length}\")\n",
        "                repeat_factor = max(1, expected_length // current_length)\n",
        "                if repeat_factor > 1:\n",
        "                    inputs[key] = np.tile(inputs[key], (repeat_factor,) + (1,) * (inputs[key].ndim - 1))\n",
        "                if len(inputs[key]) > expected_length:\n",
        "                    inputs[key] = inputs[key][:expected_length]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert {key} to numpy array: {e}\")\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qXWovz06F6yK"
      },
      "outputs": [],
      "source": [
        "def vectorizer_input(inputs):\n",
        "  inputs[\"input_user_name\"] = name_vectorizer(inputs[\"input_user_name\"])\n",
        "  inputs[\"input_user_place\"] = place_vectorizer(inputs[\"input_user_place\"])\n",
        "  inputs[\"input_user_inscription_date\"] = inscription_date_vectorizer(inputs[\"input_user_inscription_date\"])\n",
        "  inputs[\"input_art_title\"] = art_title_vectorizer(inputs[\"input_art_title\"])\n",
        "  inputs[\"input_art_author\"] = art_author_vectorizer(inputs[\"input_art_author\"])\n",
        "  inputs[\"input_art_date\"] = art_date_vectorizer(inputs[\"input_art_date\"])\n",
        "  return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KnZutwcvTpHD"
      },
      "outputs": [],
      "source": [
        "def expand_inputs(inputs):\n",
        "  inputs[\"input_user_page_views\"] = np.expand_dims(inputs[\"input_user_page_views\"], axis=-1)\n",
        "  inputs[\"input_user_followers\"] = np.expand_dims(inputs[\"input_user_followers\"], axis=-1)\n",
        "  inputs[\"input_user_follow\"] = np.expand_dims(inputs[\"input_user_follow\"], axis=-1)\n",
        "  inputs[\"input_user_favourites\"] = np.expand_dims(inputs[\"input_user_favourites\"], axis=-1)\n",
        "  inputs[\"input_user_comments_made\"] = np.expand_dims(inputs[\"input_user_comments_made\"], axis=-1)\n",
        "  inputs[\"input_user_comments_received\"] = np.expand_dims(inputs[\"input_user_comments_received\"], axis=-1)\n",
        "  inputs[\"input_art_likes\"] = np.expand_dims(inputs[\"input_art_likes\"], axis=-1)\n",
        "  inputs[\"input_art_views\"] = np.expand_dims(inputs[\"input_art_views\"], axis=-1)\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_nans_in_inputs(inputs):\n",
        "    for key, value in inputs.items():\n",
        "        if isinstance(value, np.ndarray):\n",
        "            value = tf.convert_to_tensor(value)\n",
        "        if value.dtype.is_floating:\n",
        "            if tf.math.reduce_any(tf.math.is_nan(value)):\n",
        "                inputs[key] = tf.where(tf.math.is_nan(value), tf.zeros_like(value), value)\n",
        "            else:\n",
        "                print(f\"Nessun NaN in {key}\")\n",
        "        else:\n",
        "            print(f\"{key} non è un tipo a virgola mobile, saltato.\")       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_inputs_to_tensors(inputs):\n",
        "    for key, value in inputs.items():\n",
        "        if isinstance(value, np.ndarray) or isinstance(value, list):\n",
        "            inputs[key] = tf.convert_to_tensor(value)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g6w6664r0lwY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting input_user_name from 964 to 379\n",
            "Adjusting input_user_place from 964 to 379\n",
            "Adjusting input_user_inscription_date from 964 to 379\n",
            "Adjusting input_user_page_views from 964 to 379\n",
            "Adjusting input_user_followers from 964 to 379\n",
            "Adjusting input_user_follow from 964 to 379\n",
            "Adjusting input_user_favourites from 964 to 379\n",
            "Adjusting input_user_comments_made from 964 to 379\n",
            "Adjusting input_user_comments_received from 964 to 379\n",
            "Adjusting input_art_title from 964 to 379\n",
            "Adjusting input_art_author from 964 to 379\n",
            "Adjusting input_art_likes from 964 to 379\n",
            "Adjusting input_art_views from 964 to 379\n",
            "Adjusting input_art_date from 964 to 379\n",
            "Adjusting input_pu from 964 to 379\n",
            "Adjusting input_i from 964 to 379\n",
            "Adjusting input_j from 964 to 379\n",
            "input_user_name non è un tipo a virgola mobile, saltato.\n",
            "input_user_place non è un tipo a virgola mobile, saltato.\n",
            "input_user_inscription_date non è un tipo a virgola mobile, saltato.\n",
            "Nessun NaN in input_user_page_views\n",
            "Nessun NaN in input_user_followers\n",
            "Nessun NaN in input_user_follow\n",
            "Nessun NaN in input_user_favourites\n",
            "Nessun NaN in input_user_comments_made\n",
            "Nessun NaN in input_user_comments_received\n",
            "input_art_title non è un tipo a virgola mobile, saltato.\n",
            "input_art_author non è un tipo a virgola mobile, saltato.\n",
            "input_art_likes non è un tipo a virgola mobile, saltato.\n",
            "input_art_views non è un tipo a virgola mobile, saltato.\n",
            "input_art_date non è un tipo a virgola mobile, saltato.\n",
            "Nessun NaN in input_pu\n",
            "Nessun NaN in input_i\n",
            "Nessun NaN in input_j\n"
          ]
        }
      ],
      "source": [
        "triples = create_triples(user_df, artwork_df)\n",
        "preprocessed_triples = preprocess_data(triples, preprocess_image)\n",
        "filtered_triples = [triple for triple in preprocessed_triples if triple[0].ndim == 4]\n",
        "inputs = prepare_inputs(filtered_triples, user_df)\n",
        "inputs = vectorizer_input(inputs)\n",
        "inputs = expand_inputs(inputs)\n",
        "convert_inputs_to_tensors(inputs)\n",
        "replace_nans_in_inputs(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting input_user_name\n",
            "Key: input_user_name, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_place\n",
            "Key: input_user_place, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_inscription_date\n",
            "Key: input_user_inscription_date, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_page_views\n",
            "Key: input_user_page_views, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_followers\n",
            "Key: input_user_followers, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_follow\n",
            "Key: input_user_follow, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_favourites\n",
            "Key: input_user_favourites, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_comments_made\n",
            "Key: input_user_comments_made, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_user_comments_received\n",
            "Key: input_user_comments_received, dtype: <dtype: 'float64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_art_title\n",
            "Key: input_art_title, dtype: <dtype: 'int64'>, train shape: (303, 14), test shape: (76, 14)\n",
            "Splitting input_art_author\n",
            "Key: input_art_author, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_art_likes\n",
            "Key: input_art_likes, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_art_views\n",
            "Key: input_art_views, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_art_date\n",
            "Key: input_art_date, dtype: <dtype: 'int64'>, train shape: (303, 1), test shape: (76, 1)\n",
            "Splitting input_pu\n",
            "Key: input_pu, dtype: <dtype: 'float64'>, train shape: (303, 3, 224, 224, 3), test shape: (76, 3, 224, 224, 3)\n",
            "Splitting input_i\n",
            "Key: input_i, dtype: <dtype: 'float64'>, train shape: (303, 224, 224, 3), test shape: (76, 224, 224, 3)\n",
            "Splitting input_j\n",
            "Key: input_j, dtype: <dtype: 'float64'>, train shape: (303, 224, 224, 3), test shape: (76, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test = {}, {}\n",
        "for key, value in inputs.items():\n",
        "    print(f\"Splitting {key}\")\n",
        "    indices = tf.range(start=0, limit=tf.shape(value)[0], dtype=tf.int32)\n",
        "    train_idx, test_idx = train_test_split(indices.numpy(), test_size=0.2, random_state=42) \n",
        "    train_idx = tf.convert_to_tensor(train_idx, dtype=tf.int32)\n",
        "    test_idx = tf.convert_to_tensor(test_idx, dtype=tf.int32)\n",
        "    X_train[key] = tf.gather(value, train_idx)\n",
        "    X_test[key] = tf.gather(value, test_idx)\n",
        "    print(f\"Key: {key}, dtype: {value.dtype}, train shape: {X_train[key].shape}, test shape: {X_test[key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = np.zeros((len(triples), 1))\n",
        "train_labels = labels[:303]\n",
        "test_labels = labels[303 : 303 + 76]\n",
        "curatornet_model = build_curatornet_model()\n",
        "curatornet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), loss=custom_triplet_loss, metrics=[\"accuracy\"])\n",
        "curatornet_model.fit(X_train, train_labels, epochs=10, batch_size=32)\n",
        "val_loss, val_acc = curatornet_model.evaluate(X_test, test_labels)\n",
        "print(f\"Validation loss: {val_loss}, Validation accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n"
          ]
        }
      ],
      "source": [
        "embedding_model = Model(\n",
        "    inputs=curatornet_model.input,\n",
        "    outputs=[\n",
        "        curatornet_model.get_layer(\"user_embedding\").output,\n",
        "        curatornet_model.get_layer(\"artwork_embedding\").output,\n",
        "    ],\n",
        ")\n",
        "user_embeddings, art_embeddings = embedding_model.predict(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision_at_k(actual, predicted, k):\n",
        "    actual_set = set(actual)\n",
        "    predicted_at_k = predicted[:k]\n",
        "    return len(set(predicted_at_k) & actual_set) / k\n",
        "\n",
        "\n",
        "def recall_at_k(actual, predicted, k):\n",
        "    actual_set = set(actual)\n",
        "    predicted_at_k = predicted[:k]\n",
        "    return len(set(predicted_at_k) & actual_set) / len(actual_set)\n",
        "\n",
        "\n",
        "def ndcg_at_k(actual, predicted, k):\n",
        "    actual_set = set(actual)\n",
        "    predicted_at_k = predicted[:k]\n",
        "    dcg = sum(\n",
        "        [\n",
        "            1.0 / np.log2(i + 2) if predicted_at_k[i] in actual_set else 0.0\n",
        "            for i in range(k)\n",
        "        ]\n",
        "    )\n",
        "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
        "    return dcg / idcg if idcg > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_recommendations(user_embedding, artwork_embeddings, k=10):\n",
        "    similarities = np.dot(artwork_embeddings, user_embedding)\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
        "    return top_k_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_artworks_by_popularity_score(k):\n",
        "    cleaned_artworks_df = artwork_df.sort_values(\n",
        "        \"number_of_views\", ascending=False\n",
        "    ).drop_duplicates(subset=[\"title\", \"author\"], keep=\"first\")\n",
        "    cleaned_artworks_df[\"popularity_score\"] = (\n",
        "        cleaned_artworks_df[\"number_of_views\"] * 0.2  \n",
        "        + cleaned_artworks_df[\"likes\"] * 0.5  \n",
        "        + cleaned_artworks_df[\"number_of_comments\"] * 0.3  \n",
        "    )\n",
        "    top_k_artworks = cleaned_artworks_df.nlargest(k, \"popularity_score\")\n",
        "    return top_k_artworks.index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.08, Recall: 0.04, NDCG: 0.06366688224677705\n"
          ]
        }
      ],
      "source": [
        "test = get_top_k_recommendations(user_embeddings[0], art_embeddings, k=150)\n",
        "views_set = top_k_artworks_by_popularity_score(100)\n",
        "precision = precision_at_k(views_set, test, 50)\n",
        "recall = recall_at_k(views_set, test, 50)\n",
        "ndcg = ndcg_at_k(views_set, test, 50)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, NDCG: {ndcg}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
