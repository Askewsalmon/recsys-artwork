{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h1QplZJxzqm_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Concatenate,\n",
        "    Input,\n",
        "    Embedding,\n",
        "    Lambda,\n",
        "    TextVectorization,\n",
        "    Normalization,\n",
        "    GlobalAveragePooling2D,\n",
        "    GlobalAveragePooling1D,\n",
        "    BatchNormalization,\n",
        ")\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7279pOar0b98"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/users.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/users.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m artwork_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/artworks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/users.csv'"
          ]
        }
      ],
      "source": [
        "user_df = pd.read_csv(\"../dataset/users.csv\", delimiter=\";\")\n",
        "artwork_df = pd.read_csv(\"../dataset/artworks.csv\", delimiter=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr1n5IXp9Lmj"
      },
      "outputs": [],
      "source": [
        "user_df[\"place\"] = user_df[\"place\"].astype(str)\n",
        "user_df[\"place\"] = user_df[\"place\"].fillna(\"N/A\")\n",
        "\n",
        "user_df[\"inscription_date\"] = user_df[\"inscription_date\"].astype(str)\n",
        "user_df[\"inscription_date\"] = user_df[\"inscription_date\"].fillna(\"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD6iZ2370dcV"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_url):\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = image.resize((224, 224))\n",
        "        image_array = np.array(image)\n",
        "        image_array = image_array / 255.0\n",
        "        return image_array\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Impossibile scaricare l'immagine dall'URL: {image_url}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFV80hZI0ekm"
      },
      "outputs": [],
      "source": [
        "def create_triples(user_df, artwork_df):\n",
        "    triples = []\n",
        "    for _, user_row in user_df.iterrows():\n",
        "        user_interactions = artwork_df[artwork_df[\"author\"] == user_row[\"name\"]]\n",
        "        if len(user_interactions) < 2:\n",
        "            continue\n",
        "        Pu = user_interactions.sample(frac=1).iloc[:-1]\n",
        "        i = user_interactions.sample()\n",
        "        non_interacted = artwork_df[~artwork_df[\"author\"].isin([user_row[\"name\"]])]\n",
        "        j = non_interacted.sample()\n",
        "        triples.append((Pu, i, j))\n",
        "    return triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuQsFuBI0gSE"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(triples, image_preprocessor):\n",
        "    processed_triples = []\n",
        "\n",
        "    for Pu, i, j in triples:\n",
        "        Pu_images = np.array([image_preprocessor(url) for url in Pu[\"img\"].values])\n",
        "        i_image = np.array(image_preprocessor(i[\"img\"].values[0]))\n",
        "        j_image = np.array(image_preprocessor(j[\"img\"].values[0]))\n",
        "        processed_triples.append((Pu_images, i_image, j_image, Pu, i.iloc[0], j.iloc[0]))\n",
        "\n",
        "    return processed_triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLNbF3dO0GM3"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200\n",
        "pu_dim = 400\n",
        "max_text_words = 5000\n",
        "max_comment_words = 1000\n",
        "margin = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr8OwgQR67O_"
      },
      "outputs": [],
      "source": [
        "name_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "name_vectorizer.adapt(user_df[\"name\"])\n",
        "place_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "place_vectorizer.adapt(user_df[\"place\"])\n",
        "\n",
        "inscription_date_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "inscription_date_vectorizer.adapt(user_df[\"inscription_date\"])\n",
        "\n",
        "art_title_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_title_vectorizer.adapt(artwork_df[\"title\"])\n",
        "\n",
        "art_author_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_author_vectorizer.adapt(artwork_df[\"author\"])\n",
        "\n",
        "art_date_vectorizer = TextVectorization(max_tokens=max_text_words)\n",
        "art_date_vectorizer.adapt(artwork_df[\"date\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ms3tYKp0Hut"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling=None)\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "def extract_features(image):\n",
        "    features=resnet(image)\n",
        "    features = GlobalAveragePooling2D()(features)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bQTxOgk0JT1"
      },
      "outputs": [],
      "source": [
        "def create_text_embedding(text_input, max_words, output_dim):\n",
        "    embedding = Embedding(input_dim=max_words, output_dim=output_dim)(text_input)\n",
        "    pooling_embedding = GlobalAveragePooling1D()(embedding)\n",
        "    return pooling_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aUYCDfy0LV9"
      },
      "outputs": [],
      "source": [
        "normalization_layer = Normalization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q_zSPG40Njl",
        "outputId": "fa22ea0c-eb34-4ff8-9939-fb0f6661901f"
      },
      "outputs": [],
      "source": [
        "input_user_name = Input(shape=(1,), name=\"input_user_name\")\n",
        "input_user_place = Input(shape=(1,), name=\"input_user_place\")\n",
        "input_user_inscription_date = Input(shape=(1,), name=\"input_user_inscription_date\")\n",
        "input_user_page_views = Input(shape=(1,), name=\"input_user_page_views\")\n",
        "input_user_followers = Input(shape=(1,), name=\"input_user_followers\")\n",
        "input_user_follow = Input(shape=(1,), name=\"input_user_follow\")\n",
        "input_user_favourites = Input(shape=(1,), name=\"input_user_favourites\")\n",
        "input_user_comments_made = Input(shape=(1,), name=\"input_user_comments_made\")\n",
        "input_user_comments_received = Input(shape=(1,), name=\"input_user_comments_received\")\n",
        "\n",
        "normalized_inscription_date = normalization_layer(input_user_inscription_date)\n",
        "normalized_page_views = normalization_layer(input_user_page_views)\n",
        "normalized_followers = normalization_layer(input_user_followers)\n",
        "normalized_follow = normalization_layer(input_user_follow)\n",
        "normalized_favourites = normalization_layer(input_user_favourites)\n",
        "normalized_comments_made = normalization_layer(input_user_comments_made)\n",
        "normalized_comments_received = normalization_layer(input_user_comments_received)\n",
        "\n",
        "normalized_inscription_date = Dense(embedding_dim, activation='selu')(normalized_inscription_date)\n",
        "normalized_page_views = Dense(embedding_dim, activation=\"selu\")(normalized_page_views)\n",
        "normalized_followers = Dense(embedding_dim, activation=\"selu\")(normalized_followers)\n",
        "normalized_follow = Dense(embedding_dim, activation=\"selu\")(normalized_follow)\n",
        "normalized_favourites = Dense(embedding_dim, activation=\"selu\")(normalized_favourites)\n",
        "normalized_comments_made = Dense(embedding_dim, activation=\"selu\")(\n",
        "    normalized_comments_made\n",
        ")\n",
        "normalized_comments_received = Dense(embedding_dim, activation=\"selu\")(\n",
        "    normalized_comments_received\n",
        ")\n",
        "\n",
        "user_features = Concatenate()(\n",
        "    [\n",
        "        create_text_embedding(input_user_name, max_text_words, embedding_dim),\n",
        "        create_text_embedding(input_user_place, max_text_words, embedding_dim),\n",
        "        create_text_embedding(input_user_inscription_date, max_text_words, embedding_dim),\n",
        "        normalized_page_views,\n",
        "        normalized_followers,\n",
        "        normalized_follow,\n",
        "        normalized_favourites,\n",
        "        normalized_comments_made,\n",
        "        normalized_comments_received,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zedcZS40OUT"
      },
      "outputs": [],
      "source": [
        "input_art_title = Input(shape=(8,), name=\"input_art_title\")\n",
        "input_art_author = Input(shape=(1,), name=\"input_art_author\")\n",
        "input_art_likes = Input(shape=(1,), name=\"input_art_likes\")\n",
        "input_art_views = Input(shape=(1,), name=\"input_art_views\")\n",
        "input_art_date = Input(shape=(1,), name=\"input_art_date\")\n",
        "\n",
        "art_title_embedding = create_text_embedding(\n",
        "    input_art_title, max_text_words, embedding_dim\n",
        ")\n",
        "art_author_embedding = create_text_embedding(\n",
        "    input_art_author, max_text_words, embedding_dim\n",
        ")\n",
        "art_date_embedding = create_text_embedding(\n",
        "    input_art_date, max_text_words, embedding_dim\n",
        ")\n",
        "\n",
        "art_features = Concatenate()(\n",
        "    [\n",
        "        art_title_embedding,\n",
        "        art_author_embedding,\n",
        "        normalization_layer(input_art_likes),\n",
        "        normalization_layer(input_art_views),\n",
        "        normalization_layer(input_art_date),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hxgbdqY0QW5"
      },
      "outputs": [],
      "source": [
        "def custom_reduce_sum(x, y):\n",
        "    return K.sum(x * y, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-YEp9Az0SGd"
      },
      "outputs": [],
      "source": [
        "def custom_triplet_loss(y_true, y_pred, margin=0.2):\n",
        "    score_i, score_j = tf.split(y_pred, num_or_size_splits=2, axis=-1)\n",
        "    loss = tf.maximum(0.0, margin + score_j - score_i)\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l10MY_k0UoP",
        "outputId": "1153efb9-195b-4b78-d68d-39c6e0065dd1"
      },
      "outputs": [],
      "source": [
        "input_pu = Input(shape=(3,224, 224, 3), name=\"input_pu\")\n",
        "input_i = Input(shape=(224, 224, 3), name=\"input_i\")\n",
        "input_j = Input(shape=(224, 224, 3), name=\"input_j\")\n",
        "pu_features = Lambda(\n",
        "    lambda x: K.map_fn(\n",
        "        lambda y: extract_features(y), x\n",
        "    ),\n",
        "    output_shape=(None, embedding_dim),\n",
        ")(input_pu)\n",
        "\n",
        "i_features = extract_features(input_i)\n",
        "j_features = extract_features(input_j)\n",
        "\n",
        "dense_layer_1 = Dense(embedding_dim, activation=\"selu\", name=\"dense_layer_1\")\n",
        "dense_layer_2 = Dense(embedding_dim, activation=\"selu\", name=\"dense_layer_2\")\n",
        "\n",
        "reduced_pu = Lambda(\n",
        "    lambda x: K.map_fn(\n",
        "        lambda y: dense_layer_2(dense_layer_1(y)), x\n",
        "    ),\n",
        "    output_shape=(None, embedding_dim),\n",
        ")(pu_features)\n",
        "\n",
        "\n",
        "reduced_i = dense_layer_2(dense_layer_1(i_features))\n",
        "reduced_j = dense_layer_2(dense_layer_1(j_features))\n",
        "\n",
        "concat_i= Concatenate()([reduced_i, user_features, art_features])\n",
        "concat_j= Concatenate()([reduced_j, user_features, art_features])\n",
        "\n",
        "\n",
        "dense_comb_i= Dense(embedding_dim, activation=\"selu\", name=\"dense_comb_i\")(concat_i)\n",
        "dense_comb_j= Dense(embedding_dim, activation=\"selu\", name=\"dense_comb_j\")(concat_j)\n",
        "\n",
        "\n",
        "average_pooled_pu = Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,))(\n",
        "    reduced_pu\n",
        ")\n",
        "max_pooled_pu = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim,))(\n",
        "    reduced_pu\n",
        ")\n",
        "\n",
        "pooled_pu = Concatenate()([average_pooled_pu, max_pooled_pu])\n",
        "\n",
        "\n",
        "pu_dense_1 = Dense(300, activation=\"selu\", name=\"pu_dense_1\")(pooled_pu)\n",
        "pu_dense_2 = Dense(200, activation=\"selu\", name=\"pu_dense_2\")(pu_dense_1)\n",
        "\n",
        "final_pu = Dense(200, activation=\"selu\", name=\"pu_dense_3\")(pu_dense_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp0FRbmn0VXI"
      },
      "outputs": [],
      "source": [
        "score_i = Lambda(lambda x: K.sum(x[0] * x[1], axis=1, keepdims=True))([final_pu, dense_comb_i])\n",
        "score_j = Lambda(lambda x: K.sum(x[0] * x[1], axis=1, keepdims=True))([final_pu, dense_comb_j])\n",
        "output_scores = Concatenate(axis=-1)([score_i, score_j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pSJwfxqG0YOr",
        "outputId": "7603ba90-97f3-4a2f-9402-bfa379d3c3c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_curatornet_model():\n",
        "    tf.keras.backend.clear_session()\n",
        "    curatornet = Model(\n",
        "        inputs=[\n",
        "            input_user_name,\n",
        "            input_user_place,\n",
        "            input_user_inscription_date,\n",
        "            input_user_page_views,\n",
        "            input_user_followers,\n",
        "            input_user_follow,\n",
        "            input_user_favourites,\n",
        "            input_user_comments_made,\n",
        "            input_user_comments_received,\n",
        "            input_art_title,\n",
        "            input_art_author,\n",
        "            input_art_likes,\n",
        "            input_art_views,\n",
        "            input_art_date,\n",
        "            input_pu,\n",
        "            input_i,\n",
        "            input_j,\n",
        "        ],\n",
        "        outputs=output_scores,\n",
        "    )\n",
        "    return curatornet\n",
        "\n",
        "# curatornet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.00001), loss=custom_triplet_loss)\n",
        "# curatornet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jXqMqPM0ilM"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs(processed_triples, users_df, target_image_count=3, expected_length=183):\n",
        "    inputs = {\n",
        "        \"input_user_name\": [],\n",
        "        \"input_user_place\": [],\n",
        "        \"input_user_inscription_date\": [],\n",
        "        \"input_user_page_views\": [],\n",
        "        \"input_user_followers\": [],\n",
        "        \"input_user_follow\": [],\n",
        "        \"input_user_favourites\": [],\n",
        "        \"input_user_comments_made\": [],\n",
        "        \"input_user_comments_received\": [],\n",
        "        \"input_art_title\": [],\n",
        "        \"input_art_author\": [],\n",
        "        \"input_art_likes\": [],\n",
        "        \"input_art_views\": [],\n",
        "        \"input_art_date\": [],\n",
        "        \"input_pu\": [],\n",
        "        \"input_i\": [],\n",
        "        \"input_j\": [],\n",
        "    }\n",
        "\n",
        "    for pu_images, i_image, j_image, Pu_meta, i_meta, j_meta in processed_triples:\n",
        "        user_features = users_df[users_df[\"name\"] == i_meta[\"author\"]].iloc[0]\n",
        "        if pu_images.shape[0] < target_image_count:\n",
        "            pad_width = target_image_count - pu_images.shape[0]\n",
        "            pu_images = np.pad(pu_images, ((0, pad_width), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
        "        elif pu_images.shape[0] > target_image_count:\n",
        "            pu_images = pu_images[:target_image_count]\n",
        "\n",
        "        inputs[\"input_pu\"].append(pu_images)\n",
        "        inputs[\"input_i\"].append(i_image)\n",
        "        inputs[\"input_j\"].append(j_image)\n",
        "        inputs[\"input_user_name\"].append(user_features[\"name\"])\n",
        "        inputs[\"input_user_place\"].append(user_features[\"place\"])\n",
        "        inputs[\"input_user_inscription_date\"].append(user_features[\"inscription_date\"])\n",
        "        inputs[\"input_user_page_views\"].append(user_features[\"number_page_views\"])\n",
        "        inputs[\"input_user_followers\"].append(user_features[\"number_followers\"])\n",
        "        inputs[\"input_user_follow\"].append(user_features[\"number_follow\"])\n",
        "        inputs[\"input_user_favourites\"].append(user_features[\"number_favourites\"])\n",
        "        inputs[\"input_user_comments_made\"].append(user_features[\"number_comments_made\"])\n",
        "        inputs[\"input_user_comments_received\"].append(\n",
        "            user_features[\"number_comments_receveid\"]\n",
        "        )\n",
        "        inputs[\"input_art_title\"].append(i_meta[\"title\"])\n",
        "        inputs[\"input_art_author\"].append(i_meta[\"author\"])\n",
        "        inputs[\"input_art_likes\"].append(i_meta[\"likes\"])\n",
        "        inputs[\"input_art_views\"].append(i_meta[\"number_of_views\"])\n",
        "        inputs[\"input_art_date\"].append(i_meta[\"date\"])\n",
        "\n",
        "    for key in inputs:\n",
        "        try:\n",
        "            if not isinstance(inputs[key], np.ndarray):\n",
        "                inputs[key] = np.array(inputs[key])\n",
        "\n",
        "            current_length = len(inputs[key])\n",
        "            if current_length != expected_length:\n",
        "                print(f\"Adjusting {key} from {current_length} to {expected_length}\")\n",
        "                repeat_factor = max(1, expected_length // current_length)\n",
        "                if repeat_factor > 1:\n",
        "                    inputs[key] = np.tile(inputs[key], (repeat_factor,) + (1,) * (inputs[key].ndim - 1))\n",
        "                if len(inputs[key]) > expected_length:\n",
        "                    inputs[key] = inputs[key][:expected_length]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert {key} to numpy array: {e}\")\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXWovz06F6yK"
      },
      "outputs": [],
      "source": [
        "def vectorizer_input(inputs):\n",
        "  inputs[\"input_user_name\"] = name_vectorizer(inputs[\"input_user_name\"])\n",
        "  inputs[\"input_user_place\"] = place_vectorizer(inputs[\"input_user_place\"])\n",
        "  inputs[\"input_user_inscription_date\"] = inscription_date_vectorizer(inputs[\"input_user_inscription_date\"])\n",
        "  inputs[\"input_art_title\"] = art_title_vectorizer(inputs[\"input_art_title\"])\n",
        "  inputs[\"input_art_author\"] = art_author_vectorizer(inputs[\"input_art_author\"])\n",
        "  inputs[\"input_art_date\"] = art_date_vectorizer(inputs[\"input_art_date\"])\n",
        "  return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnZutwcvTpHD"
      },
      "outputs": [],
      "source": [
        "def expand_inputs(inputs):\n",
        "  inputs[\"input_user_page_views\"] = np.expand_dims(inputs[\"input_user_page_views\"], axis=-1)\n",
        "  inputs[\"input_user_followers\"] = np.expand_dims(inputs[\"input_user_followers\"], axis=-1)\n",
        "  inputs[\"input_user_follow\"] = np.expand_dims(inputs[\"input_user_follow\"], axis=-1)\n",
        "  inputs[\"input_user_favourites\"] = np.expand_dims(inputs[\"input_user_favourites\"], axis=-1)\n",
        "  inputs[\"input_user_comments_made\"] = np.expand_dims(inputs[\"input_user_comments_made\"], axis=-1)\n",
        "  inputs[\"input_user_comments_received\"] = np.expand_dims(inputs[\"input_user_comments_received\"], axis=-1)\n",
        "  inputs[\"input_art_likes\"] = np.expand_dims(inputs[\"input_art_likes\"], axis=-1)\n",
        "  inputs[\"input_art_views\"] = np.expand_dims(inputs[\"input_art_views\"], axis=-1)\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_nans_in_inputs(inputs):\n",
        "    for key, value in inputs.items():\n",
        "        if isinstance(value, np.ndarray):\n",
        "            value = tf.convert_to_tensor(value)\n",
        "        if value.dtype.is_floating:\n",
        "            if tf.math.reduce_any(tf.math.is_nan(value)):\n",
        "                inputs[key] = tf.where(tf.math.is_nan(value), tf.zeros_like(value), value)\n",
        "            else:\n",
        "                print(f\"Nessun NaN in {key}\")\n",
        "        else:\n",
        "            print(f\"{key} non è un tipo a virgola mobile, saltato.\")       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_inputs_to_tensors(inputs):\n",
        "    for key, value in inputs.items():\n",
        "        if isinstance(value, np.ndarray) or isinstance(value, list):\n",
        "            inputs[key] = tf.convert_to_tensor(value)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHwn6JUc0kV-"
      },
      "outputs": [],
      "source": [
        "def check_input_types(inputs):\n",
        "    for key, value in inputs.items():\n",
        "        print(f\"Input key: {key}\")\n",
        "        print(f\" - Type: {type(value)}\")\n",
        "        if isinstance(value, np.ndarray):\n",
        "            print(f\" - Dtype: {value.dtype}\")\n",
        "            print(f\" - Shape: {value.shape}\")\n",
        "        else:\n",
        "            print(f\" - Not a NumPy array\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6w6664r0lwY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_user_name non è un tipo a virgola mobile, saltato.\n",
            "input_user_place non è un tipo a virgola mobile, saltato.\n",
            "input_user_inscription_date non è un tipo a virgola mobile, saltato.\n",
            "input_art_title non è un tipo a virgola mobile, saltato.\n",
            "input_art_author non è un tipo a virgola mobile, saltato.\n",
            "input_art_likes non è un tipo a virgola mobile, saltato.\n",
            "input_art_views non è un tipo a virgola mobile, saltato.\n",
            "input_art_date non è un tipo a virgola mobile, saltato.\n",
            "Nessun NaN in input_pu\n",
            "Nessun NaN in input_i\n",
            "Nessun NaN in input_j\n"
          ]
        }
      ],
      "source": [
        "triples = create_triples(user_df, artwork_df)\n",
        "preprocessed_triples = preprocess_data(triples, preprocess_image)\n",
        "inputs = prepare_inputs(preprocessed_triples, user_df)\n",
        "inputs = vectorizer_input(inputs)\n",
        "inputs = expand_inputs(inputs)\n",
        "convert_inputs_to_tensors(inputs)\n",
        "replace_nans_in_inputs(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Train Index: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  17  20  21  22\n",
            "  23  25  26  27  28  31  32  33  34  36  37  38  39  40  41  43  44  46\n",
            "  47  48  49  50  52  53  54  57  58  59  61  62  63  64  65  67  70  71\n",
            "  72  73  74  76  77  78  79  80  81  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  99 100 101 102 103 104 105 106 107 108 109 110 112\n",
            " 114 115 116 118 120 121 122 123 126 127 128 129 130 131 132 133 134 135\n",
            " 136 137 138 139 140 141 142 143 144 148 149 151 152 153 154 156 157 158\n",
            " 159 160 162 164 165 166 167 168 169 170 171 172 175 176 177 178 179 180\n",
            " 181 182]\n",
            "Test Index: [  9  15  16  18  19  24  29  30  35  42  45  51  55  56  60  66  68  69\n",
            "  75  82  97  98 111 113 117 119 124 125 145 146 147 150 155 161 163 173\n",
            " 174]\n",
            "divisione completata\n",
            "compilazione completata\n"
          ]
        }
      ],
      "source": [
        "n_splits = 5 \n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "y = np.zeros((len(triples), 1))\n",
        "fold = 1\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "for train_index, test_index in kf.split(inputs[\"input_user_name\"]):\n",
        "    print(f\"Fold {fold}\")\n",
        "    print(f\"Train Index: {train_index}\")    \n",
        "    print(f\"Test Index: {test_index}\")\n",
        "    X_train = {key: np.array(val)[train_index] for key, val in inputs.items()}\n",
        "    X_test = {key: np.array(val)[test_index] for key, val in inputs.items()}\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    print(f\"divisione completata\")\n",
        "\n",
        "    model =  build_curatornet_model()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00000005),\n",
        "        loss=custom_triplet_loss,\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    print(f\"compilazione completata\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train, epochs=10, verbose=0\n",
        "    )\n",
        "\n",
        "    print(f\"addestramento completato\")\n",
        "\n",
        "    val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print(\n",
        "        f\"Fold {fold} - Validation Loss: {val_loss} - Validation Accuracy: {val_accuracy}\"\n",
        "    )\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    fold += 1\n",
        "\n",
        "average_val_loss = np.mean(val_losses)\n",
        "print(\n",
        "    f\"Mean Validation Loss after {n_splits}-Fold Cross Validation: {average_val_loss}\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
