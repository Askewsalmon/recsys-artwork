{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759\n",
      "766\n"
     ]
    }
   ],
   "source": [
    "artworks = pd.read_csv(\"../dataset/artworks.csv\", delimiter=\";\")\n",
    "users = pd.read_csv(\"../dataset/users.csv\", delimiter=\";\")\n",
    "artworks.fillna(-1, inplace=True)\n",
    "users.fillna(-1, inplace=True)\n",
    "artworks = artworks.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "users = users.drop_duplicates(subset=\"name\", keep=\"first\")\n",
    "user_to_index = {username: idx for idx, username in enumerate(users[\"name\"])}\n",
    "artwork_to_index = {title: idx for idx, title in enumerate(artworks[\"title\"])}\n",
    "\n",
    "users[\"user_id\"] = users[\"name\"].map(user_to_index)\n",
    "artworks[\"artwork_id\"] = artworks[\"title\"].map(artwork_to_index)\n",
    "\n",
    "print(len(users))\n",
    "print(len(artworks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image = preprocess(image)\n",
    "            return image.unsqueeze(0)\n",
    "        else:\n",
    "            print(f\"Errore nel caricamento dell'immagine dall'URL: {image_url}\")\n",
    "            return torch.zeros(\n",
    "                (1, 3, 224, 224)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel preprocessamento dell'immagine dall'URL {image_url}: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LorenzoStancato\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "\n",
    "def extract_visual_features_from_url(image_url):\n",
    "    image = preprocess_image(image_url)\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(image)\n",
    "    return features.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "num_users = len(user_to_index) +1\n",
    "num_artworks = len(artwork_to_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0779, -0.0214, -0.0793,  ..., -0.0090,  0.0213,  0.0261],\n",
       "        [ 0.0302,  0.0659, -0.0661,  ..., -0.0346,  0.0863,  0.0772],\n",
       "        [ 0.0437, -0.0020, -0.0599,  ..., -0.0176, -0.0463,  0.0286],\n",
       "        ...,\n",
       "        [-0.0610, -0.0388,  0.0544,  ..., -0.0848, -0.0380,  0.0151],\n",
       "        [-0.0023, -0.0462,  0.0405,  ...,  0.0810,  0.0745,  0.0223],\n",
       "        [-0.0595, -0.0095,  0.0497,  ..., -0.0826,  0.0325,  0.0034]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings = torch.nn.Embedding(num_users, embedding_dim)\n",
    "artwork_embeddings = torch.nn.Embedding(num_artworks, embedding_dim)\n",
    "user_social_embeddings = torch.nn.Embedding(num_users+7, embedding_dim)\n",
    "artwork_similarity_embeddings = torch.nn.Embedding(num_artworks, embedding_dim)\n",
    "visual_embedding_layer = nn.Linear(2048, embedding_dim)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(user_embeddings.weight)\n",
    "torch.nn.init.xavier_uniform_(artwork_embeddings.weight)\n",
    "torch.nn.init.xavier_uniform_(user_social_embeddings.weight)\n",
    "torch.nn.init.xavier_uniform_(artwork_similarity_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalized_decay(k, a, b):\n",
    "    a_tensor = torch.tensor(a)\n",
    "    k_tensor = torch.tensor(k - 1)\n",
    "    return 1 / (torch.exp(a_tensor * k_tensor) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(user, artwork, past_artworks, a, b, artwork_visual_features):\n",
    "    user_emb = user_embeddings(user)\n",
    "    artwork_emb = artwork_embeddings(artwork)\n",
    "    similarity_emb = artwork_similarity_embeddings(artwork)\n",
    "\n",
    "    visual_features = artwork_visual_features\n",
    "    visual_emb = visual_embedding_layer(visual_features)\n",
    "\n",
    "    long_term_pref = torch.sum(user_emb * (artwork_emb + visual_emb), dim=-1)\n",
    "\n",
    "    short_term_sim = 0\n",
    "    for k, past_artwork in enumerate(past_artworks, 1):\n",
    "        past_artwork_tensor = torch.tensor([past_artwork])\n",
    "        past_artwork_emb = artwork_embeddings(past_artwork_tensor)\n",
    "        past_similarity_emb = artwork_similarity_embeddings(past_artwork_tensor)\n",
    "        decay_factor = personalized_decay(k, a, b)\n",
    "        short_term_sim += decay_factor * torch.sum(\n",
    "            (past_artwork_emb + past_similarity_emb) * (artwork_emb + similarity_emb),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    user_social_emb = user_social_embeddings(user)\n",
    "    artwork_owner_emb = user_social_embeddings(artwork)\n",
    "    social_sim = torch.sum(user_social_emb * artwork_owner_emb, dim=-1)\n",
    "\n",
    "    score = long_term_pref + short_term_sim + social_sim\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_bpr_loss(pos_score, neg_score):\n",
    "    return -torch.mean(F.logsigmoid(pos_score - neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions(users_df, artworks_df, num_past_works=2):\n",
    "    interactions = []\n",
    "\n",
    "    for _, user_row in users_df.iterrows():\n",
    "        user_id = user_row[\"user_id\"]\n",
    "        user_name = user_row[\"name\"]\n",
    "        non_user_artworks = artworks_df[artworks_df[\"author\"] != user_name]\n",
    "        user_artworks = artworks_df[artworks_df[\"author\"] == user_name]\n",
    "\n",
    "        if len(non_user_artworks) > 0 and len(user_artworks) > 0:\n",
    "            pos_artwork = non_user_artworks.sample(1).iloc[0][\"artwork_id\"]\n",
    "            neg_artwork = non_user_artworks.sample(1).iloc[0][\"artwork_id\"]\n",
    "            past_artworks = list(\n",
    "                user_artworks[\"artwork_id\"].sample(\n",
    "                    min(len(user_artworks), num_past_works)\n",
    "                )\n",
    "            )\n",
    "            interactions.append((user_id, pos_artwork, neg_artwork, past_artworks))\n",
    "\n",
    "    return interactions\n",
    "\n",
    "interactions = create_interactions(users, artworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_data(interactions, idx):\n",
    "    user_id, pos_artwork_id, neg_artwork_id, past_artwork_ids = interactions[idx]\n",
    "    return user_id, pos_artwork_id, neg_artwork_id, past_artwork_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_generator(interactions):\n",
    "    for interaction in interactions:\n",
    "        yield get_interaction_data(interactions, interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions_length(interactions):\n",
    "    return len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    users, pos_artworks, neg_artworks, past_artworks = zip(*batch)\n",
    "    return (\n",
    "        torch.tensor(users),\n",
    "        torch.tensor(pos_artworks),\n",
    "        torch.tensor(neg_artworks),\n",
    "        past_artworks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(interactions, batch_size=64, shuffle=True):\n",
    "    data_length = get_interactions_length(interactions)\n",
    "    indices = list(range(data_length))\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    def data_generator():\n",
    "        batch = []\n",
    "        for idx in indices:\n",
    "            batch.append(get_interaction_data(interactions, idx))\n",
    "            if len(batch) == batch_size:\n",
    "                yield custom_collate_fn(batch)\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield custom_collate_fn(batch)\n",
    "\n",
    "    return data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, learning_rate, interactions, a, b):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": user_embeddings.parameters()},\n",
    "            {\"params\": artwork_embeddings.parameters()},\n",
    "            {\"params\": user_social_embeddings.parameters()},\n",
    "            {\"params\": artwork_similarity_embeddings.parameters()},\n",
    "            {\"params\": visual_embedding_layer.parameters()},\n",
    "        ],\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        total_loss = 0\n",
    "        data_loader = create_data_loader(interactions)\n",
    "        for batch in data_loader:\n",
    "            users, pos_artworks, neg_artworks, past_artworks = batch\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            for i in range(len(users)):\n",
    "                user = users[i].unsqueeze(0)\n",
    "                pos_artwork = pos_artworks[i].unsqueeze(0)\n",
    "                neg_artwork = neg_artworks[i].unsqueeze(0)\n",
    "                past_artwork = past_artworks[i]\n",
    "                pos_artwork_row = artworks.loc[\n",
    "                    artworks[\"artwork_id\"] == pos_artwork.item()\n",
    "                ]\n",
    "                neg_artwork_row = artworks.loc[\n",
    "                    artworks[\"artwork_id\"] == neg_artwork.item()\n",
    "                ]\n",
    "                if pos_artwork_row.empty or neg_artwork_row.empty:\n",
    "                    continue\n",
    "                pos_visual_features = extract_visual_features_from_url(\n",
    "                    pos_artwork_row[\"img\"].values[0]\n",
    "                )\n",
    "                neg_visual_features = extract_visual_features_from_url(\n",
    "                    neg_artwork_row[\"img\"].values[0]\n",
    "                )\n",
    "                pos_score = compute_score(\n",
    "                    user, pos_artwork, past_artwork, a, b, pos_visual_features\n",
    "                )\n",
    "                neg_score = compute_score(\n",
    "                    user, neg_artwork, past_artwork, a, b, neg_visual_features\n",
    "                )\n",
    "                loss = s_bpr_loss(pos_score, neg_score)\n",
    "                batch_loss += loss\n",
    "            batch_loss /= len(users)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(epochs=10, learning_rate=0.001, interactions=interactions, a=0.1, b=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_has_interacted(user_id, artwork_id, interactions):\n",
    "    for interaction in interactions:\n",
    "        if (\n",
    "            interaction[0] == user_id\n",
    "            and interaction[1] == artwork_id\n",
    "        ):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_artworks(user_id, interactions):\n",
    "    past_artworks = []\n",
    "    for interaction in interactions:\n",
    "        if interaction[0] == user_id:\n",
    "            past_artworks.append(interaction[1])\n",
    "    return past_artworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_id, interactions, k):\n",
    "    user_tensor = torch.tensor([user_id]).long()\n",
    "    artwork_scores = []\n",
    "\n",
    "    for _, artwork_row in artworks.iterrows():\n",
    "        artwork_id = artwork_row[\"artwork_id\"]\n",
    "\n",
    "        if user_has_interacted(user_id, artwork_id, interactions):\n",
    "            continue\n",
    "\n",
    "        artwork_tensor = torch.tensor([artwork_id]).long()\n",
    "\n",
    "        artwork_visual_features = extract_visual_features_from_url(artwork_row[\"img\"])\n",
    "\n",
    "        past_artworks = get_past_artworks(\n",
    "            user_id, interactions\n",
    "        )  \n",
    "        score = compute_score(\n",
    "            user_tensor,\n",
    "            artwork_tensor,\n",
    "            past_artworks,\n",
    "            a=0.1,\n",
    "            b=0.5,\n",
    "            artwork_visual_features=artwork_visual_features,\n",
    "        )\n",
    "        artwork_scores.append((artwork_id, score.item()))\n",
    "    top_recommendations = sorted(artwork_scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "    return [artwork_id for artwork_id, _ in top_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_artworks_by_popularity_score(k):\n",
    "    artworks[\"popularity_score\"] = (\n",
    "        artworks[\"number_of_views\"] * 0.2\n",
    "        + artworks[\"likes\"] * 0.5\n",
    "        + artworks[\"number_of_comments\"] * 0.3\n",
    "    )\n",
    "    top_k_artworks = artworks.nlargest(k, \"popularity_score\")\n",
    "    return top_k_artworks.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = predicted[:k]\n",
    "    return len(set(predicted_at_k) & actual_set) / k\n",
    "\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = predicted[:k]\n",
    "    return len(set(predicted_at_k) & actual_set) / len(actual_set)\n",
    "\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = predicted[:k]\n",
    "    dcg = sum(\n",
    "        [\n",
    "            1.0 / np.log2(i + 2) if predicted_at_k[i] in actual_set else 0.0\n",
    "            for i in range(k)\n",
    "        ]\n",
    "    )\n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 0, 237, 614, 128, 80, 580, 475, 156, 322, 617, 381, 424, 488, 652, 30, 139, 524, 474, 731, 6, 83, 380, 73, 378, 464, 612, 416, 97, 419, 697, 269, 224, 253, 674, 748, 453, 632, 158, 194, 433, 510, 396, 622, 350, 384, 177, 81, 342, 459, 695, 174, 249, 715, 304, 636, 107, 48, 121, 523, 339, 568, 235, 112, 627, 154, 728, 675, 319, 264, 561, 472, 250, 657, 305, 710, 329, 185, 759, 23, 247, 43, 346, 100, 96, 324, 398, 50, 458, 283, 621, 490, 521, 60, 17, 691, 164, 291, 357, 37, 25, 111, 541, 724, 583, 677, 649, 88, 191, 289, 415, 3, 316, 656, 311, 457, 144, 123, 709, 683, 181, 412, 101, 281, 720, 566, 349, 563, 239, 562, 400, 331, 666, 303, 551, 193, 201, 560, 286, 295, 444, 740, 630, 353, 150, 160, 758, 550, 557, 445, 356, 654, 34, 565, 672, 738, 440, 597, 198, 296, 624, 741, 341, 212, 216, 306, 168, 279, 233, 540, 573, 504, 483, 219, 375, 608, 737, 108, 146, 671, 385, 70, 221, 454, 187, 465, 231, 173, 660, 589, 761, 712, 114, 567, 14, 512, 274, 155, 606, 450]\n"
     ]
    }
   ],
   "source": [
    "test = recommend_for_user(0, interactions, 200)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08, Recall: 0.04, NDCG: 0.06225101582759056\n"
     ]
    }
   ],
   "source": [
    "views_set = top_k_artworks_by_popularity_score(100)\n",
    "precision = precision_at_k(views_set, test, 50)\n",
    "recall = recall_at_k(views_set, test, 50)\n",
    "ndcg = ndcg_at_k(views_set, test, 50)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, NDCG: {ndcg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
